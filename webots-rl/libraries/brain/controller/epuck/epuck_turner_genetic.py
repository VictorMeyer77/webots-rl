"""
Genetic controller for the e-puck robot.

This module defines `EpuckTurnerGenetic`, a specialization of `EpuckTurner`
that replays a pre-evolved discrete action sequence (`model.actions`)
generated by a genetic algorithm trainer. It supports two modes:

1. Deployment (`run`): Sequentially applies actions each simulation step.
2. Training bridge (`train`): Acts as the controller-side peer during
"""

import numpy as np
from brain.controller.epuck import EpuckTurner
from brain.utils.logger import logger
from controller import Robot


class EpuckTurnerGenetic(EpuckTurner):
    """
    Genetic replay controller for the e-puck.

    Attributes:
        action_index (int): Current position within `model.actions`.
    """

    action_index: int

    def __init__(self, robot: Robot, timestep: int, max_speed: float):
        """
        Initialize the genetic controller.

        Args:
            robot (Robot): Webots robot instance.
            timestep (int): Simulation basic time step in milliseconds.
            max_speed (float): Maximum wheel speed to pass to base class.
        """
        super().__init__(robot=robot, timestep=timestep, max_speed=max_speed)
        self.action_index = 0

    def policy(self, _observation: dict) -> int:
        """
        Return the next discrete action from the evolved action sequence.

        Args:
            _observation (dict): Ignored; actions are precomputed.

        Returns:
            int: The current action cast to Python int.
        """
        action = self.model.actions[self.action_index]
        return int(action)

    def run(self) -> None:
        """
        Deployment loop: applies actions until simulation termination.

        Each step:
          1. Calls `step()` (base class sensing + acting).
          2. Increments `action_index`.
          3. Logs current step.

        Terminates when `Robot.step` returns -1.
        """
        while self.robot.step(self.timestep) != -1:
            self.step()
            self.action_index += 1
            logger().debug(f"Step index: {self.action_index}")

    def train(self) -> None:
        """
        Controller-side loop used during genetic evaluation.

        Notes:
            The supervisor manages reward calculation externally.
        """
        step_index = 0
        sync = False
        actions = np.array([])

        while self.robot.step(self.timestep) != -1:

            self.queue.clear_buffer()

            if step_index > 0 and step_index >= actions.size:
                continue

            # (1) Initial synchronization handshake on the very first step.
            if not sync:
                if not self.queue.search_message("sync"):
                    continue
                else:
                    self.queue.send({"ack": 1})
                    sync = True
                    logger().debug("Synchronization with supervisor successful.")

            # (2) Await actions.
            if actions.size == 0:
                action_messages = self.queue.search_message("actions")
                if not action_messages:
                    continue
                else:
                    actions = np.array(action_messages[0]["actions"])
                    logger().debug(f"Received actions {actions}")

            # (3) Execute action and notify step completion.
            self.act(actions[step_index])
            self.queue.send({"step": step_index})

            # (4) Prepare for next iteration.
            step_index += 1
